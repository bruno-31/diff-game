{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:01:47.611477Z",
     "start_time": "2018-05-04T11:01:45.303836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.contrib.kfac.python.ops.utils import fwd_gradients\n",
    "import seaborn as sns\n",
    "from utils import nn_l2_mean\n",
    "slim = tf.contrib.slim\n",
    "ds = tf.contrib.distributions\n",
    "# from universal_divergence import estimate\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:01:47.865794Z",
     "start_time": "2018-05-04T11:11:38.002Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generator and discriminator architectures\n",
    "\n",
    "(same architecture as proposed in google brain paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:02:05.606779Z",
     "start_time": "2018-05-04T11:02:05.596662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim=2, n_hidden=384, n_layer=6):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        h = slim.stack(z, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        x = slim.fully_connected(h, output_dim, activation_fn=None)\n",
    "    return x\n",
    "\n",
    "def discriminator(x, n_hidden=384, n_layer=6, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        h = slim.stack(x, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=None)\n",
    "    return log_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:02:06.196930Z",
     "start_time": "2018-05-04T11:02:06.190765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_mog(batch_size, n_mixture=16, std=0.2):\n",
    "    x = np.linspace(-4.5,4.5,4)\n",
    "    xs, ys = np.meshgrid(x, x)\n",
    "    xs, ys = xs.flatten(), ys.flatten()\n",
    "    cat = ds.Categorical(tf.zeros(n_mixture))\n",
    "    comps = [ds.MultivariateNormalDiag([xi, yi], [std, std]) for xi, yi in zip(xs.ravel(), ys.ravel())]\n",
    "    data = ds.Mixture(cat, comps)\n",
    "    return data.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:02:06.860092Z",
     "start_time": "2018-05-04T11:02:06.854520Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=512,\n",
    "    learning_rate_1=1e-3,\n",
    "    learning_rate_2=1e-4,\n",
    "    beta1=0.5,\n",
    "    epsilon=1e-8,\n",
    "    max_iter=20000,\n",
    "    viz_every=2000,\n",
    "    z_dim=256,\n",
    "    x_dim=2,\n",
    "    optimizer='extra-rmsprop', # rmsprop sgd sga\n",
    "    reg_w=10.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Symplectic gradient adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model and training ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T13:35:28.273627Z",
     "start_time": "2018-05-04T13:35:25.789016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra-rmsprop\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=(), name=\"lr_pl\")\n",
    "\n",
    "data = sample_mog(params['batch_size'])\n",
    "noise = ds.Normal(tf.zeros(params['z_dim']), tf.ones(params['z_dim'])).sample(params['batch_size'])\n",
    "\n",
    "# Construct generator and discriminator nets\n",
    "with slim.arg_scope([slim.fully_connected], weights_initializer=tf.orthogonal_initializer(gain=1.)):\n",
    "    samples = generator(noise, output_dim=params['x_dim'])\n",
    "    real_score = discriminator(data)\n",
    "    fake_score = discriminator(samples, reuse=True)\n",
    "    \n",
    "# Saddle objective    \n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=real_score, labels=tf.ones_like(real_score)) +\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.zeros_like(fake_score)))\n",
    "\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "\n",
    "if params['optimizer'] == 'sgd':\n",
    "    print('sgd')\n",
    "    g_train_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    d_train_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    d_train_op = d_train_opt.minimize(loss, var_list=disc_vars)\n",
    "    g_train_op = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "    train_op = tf.group(g_train_op, d_train_op)\n",
    "    \n",
    "elif params['optimizer'] == 'extra-sgd':\n",
    "    print('extra-sgd')\n",
    "    variables = disc_vars + gen_vars\n",
    "\n",
    "    d_grads = tf.gradients(loss, disc_vars)\n",
    "    g_grads = tf.gradients(-loss, gen_vars)\n",
    "    grads = d_grads + g_grads\n",
    "    \n",
    "    varcopy_updates = []\n",
    "    var_updates = []\n",
    "    for grad, varcopy, var in zip(grads, variables_copy, variables):\n",
    "        varcopy_updates.append(varcopy.assign(var))\n",
    "        var_updates.append(var.assign(var - params['learning_rate_1'] * grad))\n",
    "    update = varcopy_updates + var_updates\n",
    "    train_op_inter = tf.group(*update)\n",
    "    \n",
    "    var_updates = []\n",
    "    for grad, varcopy, var in zip(grads, variables_copy, variables):\n",
    "        var_updates.append(var.assign(varcopy - params['learning_rate_2'] * grad))\n",
    "    \n",
    "    with tf.control_dependencies([train_op_inter]):\n",
    "        train_op = tf.group(*var_updates)\n",
    "        \n",
    "elif params['optimizer'] == 'rmsprop':\n",
    "    print('rmsprop')\n",
    "    g_train_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    d_train_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    d_train_op = d_train_opt.minimize(loss, var_list=disc_vars)\n",
    "    g_train_op = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "    train_op = tf.group(g_train_op, d_train_op)\n",
    "    \n",
    "elif params['optimizer'] == 'extra-rmsprop':\n",
    "    print('extra-rmsprop')  \n",
    "    '''\n",
    "    1) copy w to wcopy\n",
    "    2) compute grad(loss,w) and update w \n",
    "    3) compute grad(loss,wprim)\n",
    "    4) restore w with wcopy\n",
    "    4) apply gradprim to w\n",
    "    '''\n",
    "    with tf.variable_scope('var_prim'):\n",
    "        disc_vprim = [tf.Variable(var.initialized_value()) for var in disc_vars]\n",
    "        gen_vprim = [tf.Variable(var.initialized_value()) for var in gen_vars]\n",
    "    \n",
    "    varprim = disc_vprim + gen_vprim\n",
    "    variables = disc_vars + gen_vars\n",
    "\n",
    "    copy_w_to_wprim = [wprim.assign(w) for wprim,w in zip(varprim,variables)] # assign w to wprim\n",
    "    assign = [w.assign(wprim) for wprim,w in zip(varprim,variables)] # assign w to wprim\n",
    "    \n",
    "    optimizer1 = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    optimizer2 = tf.train.RMSPropOptimizer(learning_rate)\n",
    "     \n",
    "    d_grads = tf.gradients(loss, disc_vars)\n",
    "    g_grads = tf.gradients(-loss, gen_vars)\n",
    "    grads = d_grads + g_grads\n",
    "    \n",
    "    g_w = [(g, v)for (g, v) in zip(grads, variables)]\n",
    "\n",
    "#     with tf.control_dependencies(copy_w_to_wprim):\n",
    "    first_step = optimizer1.apply_gradients(g_w)  # update w with grad_w\n",
    "\n",
    "#     with tf.control_dependencies([first_step]):  # compute gradprim\n",
    "    g_wprim = [(g, v) for (g, v) in zip(grads, varprim)]\n",
    "    train_op = optimizer2.apply_gradients(g_wprim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T13:33:35.750174Z",
     "start_time": "2018-05-04T13:42:40.495Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "# config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T13:33:35.738544Z",
     "start_time": "2018-05-04T13:25:26.190012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xmax = 3\n",
    "fs = []\n",
    "frames = []\n",
    "np_samples = []\n",
    "n_batches_viz = 10\n",
    "viz_every = params['viz_every']\n",
    "nn_every = 200\n",
    "y_ref = sess.run( data)\n",
    "nn_dist = []\n",
    "nn_kl =[]\n",
    "for i in tqdm(range(params['max_iter']+1)):\n",
    "#     f, _, _ = sess.run([[loss], g_train_op, d_train_op])\n",
    "#     f, _= sess.run([[loss], train_op])\n",
    "    sess.run(copy_w_to_wprim)\n",
    "#     print('var avant ', sess.run(variables)[0][0][:5])\n",
    "#     print('varprim avant', sess.run(varprim)[0][0][:5])\n",
    "    feed_dict = {learning_rate: lr}\n",
    "    sess.run(first_step, feed_dict=feed_dict)\n",
    "#     print('var apres opt1', sess.run(variables)[0][0][:5])\n",
    "#     print('varprim apres opt1', sess.run(varprim)[0][0][:5])\n",
    "    sess.run(train_op, feed_dict=feed_dict)\n",
    "    sess.run(assign)\n",
    "#     print('var apres opt2', sess.run(variables)[0][0][:5])\n",
    "#     print('varprim apres opt2', sess.run(varprim)[0][0][:5])\n",
    "#     fs.append(f)\n",
    "    if (i) % viz_every == 0:\n",
    "        np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "        xx, yy = sess.run([samples, data])\n",
    "        fig = figure(figsize=(5,5))\n",
    "        scatter(xx[:, 0], xx[:, 1], c='r',edgecolor='none',s=10)\n",
    "        scatter(yy[:, 0], yy[:, 1], c='g', edgecolor='none',s=10)\n",
    "        axis('off')\n",
    "        show()\n",
    "        \n",
    "#     if (i) % nn_every == 0:\n",
    "#         x = np.vstack([sess.run(samples) for _ in range(n_batches_viz)])\n",
    "#         l2nn = nn_l2_mean(x,y_ref)\n",
    "#         kl =estimate(x, y_ref,k=1)\n",
    "#         nn_dist.append(l2nn)\n",
    "#         nn_kl.append(kl)\n",
    "#         print('dist = ', l2nn)\n",
    "#         print('kl = ', kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T13:34:42.398536Z",
     "start_time": "2018-05-04T13:33:37.453224Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_samples_ = np_samples[::1]\n",
    "cols = len(np_samples_)\n",
    "bg_color  = sns.color_palette('Greens', n_colors=256)[0]\n",
    "figure(figsize=(2*cols, 2))\n",
    "for i, samps in enumerate(np_samples_):\n",
    "    if i == 0:\n",
    "        ax = subplot(1,cols,1)\n",
    "    else:\n",
    "        subplot(1,cols,i+1, sharex=ax, sharey=ax)\n",
    "#     ax2 = sns.kdeplot(samps[:, 0], samps[:, 1], shade=True, cmap='Greens', n_levels=20, clip=[[-6,6]]*2)\n",
    "    ax2 = sns.kdeplot(samps[:, 0], samps[:, 1], shade=True, cmap='coolwarm',bw=.40, n_levels=20, clip=[[-6,6]]*2)\n",
    "\n",
    "#     ax2.set_axis_bgcolor(bg_color)\n",
    "    xticks([]); yticks([])\n",
    "    title('step %d'%(i*viz_every))\n",
    "gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T12:10:25.257943Z",
     "start_time": "2018-05-04T12:19:38.694Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(nn_dist)\n",
    "plt.semilogy(nn_kl)\n",
    "plt.legend(['kl','l2 nearest neigbhors'])\n",
    "xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('plot_con_kl',nn_kl)\n",
    "np.save('plot_con_nn',nn_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "anomaly-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
