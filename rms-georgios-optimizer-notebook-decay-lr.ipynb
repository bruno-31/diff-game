{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.contrib.kfac.python.ops.utils import fwd_gradients\n",
    "import seaborn as sns\n",
    "from utils import nn_l2_mean\n",
    "slim = tf.contrib.slim\n",
    "ds = tf.contrib.distributions\n",
    "from universal_divergence import estimate\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generator and discriminator architectures\n",
    "\n",
    "(same architecture as proposed in google brain paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim=2, n_hidden=384, n_layer=6):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        h = slim.stack(z, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        x = slim.fully_connected(h, output_dim, activation_fn=None)\n",
    "    return x\n",
    "\n",
    "def discriminator(x, n_hidden=384, n_layer=6, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        h = slim.stack(x, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=None)\n",
    "    return log_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_mog(batch_size, n_mixture=16, std=0.2):\n",
    "    x = np.linspace(-4.5,4.5,4)\n",
    "    xs, ys = np.meshgrid(x, x)\n",
    "    xs, ys = xs.flatten(), ys.flatten()\n",
    "    cat = ds.Categorical(tf.zeros(n_mixture))\n",
    "    comps = [ds.MultivariateNormalDiag([xi, yi], [std, std]) for xi, yi in zip(xs.ravel(), ys.ravel())]\n",
    "    data = ds.Mixture(cat, comps)\n",
    "    return data.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=512,\n",
    "    start_learning_rate=1e-4,\n",
    "    beta1=0.5,\n",
    "    epsilon=1e-8,\n",
    "    max_iter=20000,\n",
    "    viz_every=2000,\n",
    "    z_dim=256,\n",
    "    x_dim=2,\n",
    "    optimizer='rms-georgios', # rmsprop sgd sga\n",
    "    lambd=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Symplectic gradient adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model and training ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms-georgios\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data = sample_mog(params['batch_size'])\n",
    "noise = ds.Normal(tf.zeros(params['z_dim']), tf.ones(params['z_dim'])).sample(params['batch_size'])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=(), name=\"lr_pl\")\n",
    "\n",
    "# Construct generator and discriminator nets\n",
    "with slim.arg_scope([slim.fully_connected], weights_initializer=tf.orthogonal_initializer(gain=1.)):\n",
    "    samples = generator(noise, output_dim=params['x_dim'])\n",
    "    real_score = discriminator(data)\n",
    "    fake_score = discriminator(samples, reuse=True)\n",
    "    \n",
    "# Saddle objective    \n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=real_score, labels=tf.ones_like(real_score)) +\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.zeros_like(fake_score)))\n",
    "\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "\n",
    "if params['optimizer'] == 'rmsprop':\n",
    "    print('sgd')\n",
    "    g_train_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    d_train_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    d_train_op = d_train_opt.minimize(loss, var_list=disc_vars)\n",
    "    g_train_op = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "    train_op = tf.group(g_train_op, d_train_op)\n",
    "\n",
    "elif params['optimizer'] == 'sgd':    \n",
    "    d_train_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    g_train_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    d_train_op = d_train_opt.minimize(loss, var_list=disc_vars)\n",
    "    g_train_op = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "    train_op = tf.group(d_train_op,g_train_op)\n",
    "\n",
    "elif params['optimizer'] == 'georgios': \n",
    "    print('georgios')\n",
    "\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(params['disc_learning_rate'])\n",
    "    d_grads = tf.gradients(loss, disc_vars)\n",
    "    g_grads = tf.gradients(-loss, gen_vars)\n",
    "    \n",
    "    variables = disc_vars + gen_vars\n",
    "    grads = d_grads + g_grads\n",
    "    \n",
    "    var_updates = []\n",
    "    for grad, var in zip(grads, variables):\n",
    "        var_updates.append(var.assign_sub(params['disc_learning_rate'] * grad + params['lambd'] * var))\n",
    "    train_op = tf.group(*var_updates)\n",
    "    \n",
    "elif params['optimizer'] == 'rms-georgios': \n",
    "    print('rms-georgios')\n",
    "#     d_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "#     g_opt = tf.train.RMSPropOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, use_locking=True)\n",
    "    \n",
    "    d_grads = tf.gradients(loss, disc_vars)\n",
    "    g_grads = tf.gradients(-loss, gen_vars)\n",
    "    \n",
    "    variables = disc_vars + gen_vars\n",
    "    grads = d_grads + g_grads\n",
    "    \n",
    "    apply_vec = [(g + params['lambd'] * v, v)for (g, v) in zip(grads, variables)]\n",
    "    \n",
    "    with tf.control_dependencies([g for (g, v) in apply_vec]):\n",
    "        train_op = optimizer.apply_gradients(apply_vec)\n",
    "    \n",
    "#     with tf.control_dependencies([g for (g, v) in apply_vec]):\n",
    "#         train_op = optimizer.apply_gradients(apply_vec)\n",
    "    \n",
    "\n",
    "# elif params['optimizer'] == 'consensus': \n",
    "#     print('consensus')\n",
    "#     d_opt = tf.train.RMSPropOptimizer(learning_rate=params['disc_learning_rate'])\n",
    "#     g_opt = tf.train.RMSPropOptimizer(learning_rate=params['gen_learning_rate'])\n",
    "    \n",
    "#     optimizer = tf.train.RMSPropOptimizer(params['disc_learning_rate'], use_locking=True)\n",
    "\n",
    "    \n",
    "# #     dvs = d_opt.compute_gradients(loss, var_list=disc_vars)\n",
    "# #     gvs = g_opt.compute_gradients(-loss, var_list=gen_vars)\n",
    "    \n",
    "#     d_grads = tf.gradients(loss, disc_vars)\n",
    "#     g_grads = tf.gradients(-loss, gen_vars)\n",
    "    \n",
    "#     variables = disc_vars + gen_vars\n",
    "#     grads = d_grads + g_grads\n",
    "    \n",
    "\n",
    "#     # Reguliarizer\n",
    "#     reg = 0.5 * sum(tf.reduce_sum(tf.square(g)) for g in grads)\n",
    "#     # Jacobian times gradiant\n",
    "#     Jgrads = tf.gradients(reg, variables)\n",
    "    \n",
    "#     apply_vec = [(g + params['reg_w'] * Jg, v)for (g, Jg, v) in zip(grads, Jgrads, variables) if Jg is not None]\n",
    "    \n",
    "#     with tf.control_dependencies([g for (g, v) in apply_vec]):\n",
    "#         train_op = optimizer.apply_gradients(apply_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-866d519a6112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# config = tf.ConfigProto(device_count={'GPU': 0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# config.gpu_options.allow_growth = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/anomaly-detection/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_pruned_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_nesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/anomaly-detection/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    620\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewDeprecatedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/anomaly-detection/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "# config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "# config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmax = 3\n",
    "fs = []\n",
    "frames = []\n",
    "np_samples = []\n",
    "n_batches_viz = 10\n",
    "viz_every = params['viz_every']\n",
    "nn_every = 200\n",
    "y_ref = sess.run( data)\n",
    "nn_dist = []\n",
    "nn_kl =[]\n",
    "step_lr = 1\n",
    "for i in tqdm(range(params['max_iter']+1)):\n",
    "#     f, _, _ = sess.run([[loss], g_train_op, d_train_op])\n",
    "    feed_dict = {learning_rate: params['start_learning_rate']}\n",
    "    f, _= sess.run([[loss], train_op], feed_dict=feed_dict)\n",
    "    step_lr += 1\n",
    "\n",
    "\n",
    "    fs.append(f)\n",
    "    if (i) % viz_every == 0:\n",
    "        np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "        xx, yy = sess.run([samples, data])\n",
    "        fig = figure(figsize=(5,5))\n",
    "        scatter(xx[:, 0], xx[:, 1], c='r', edgecolor='none',s=10)\n",
    "        scatter(yy[:, 0], yy[:, 1], c='g', edgecolor='none',s=10)\n",
    "#         plt.xlim([-5.5, 5.5])\n",
    "#         plt.ylim([-5.5, 5.5])\n",
    "        axis('off')\n",
    "        show()\n",
    "        \n",
    "#     if (i) % nn_every == 0:\n",
    "#         x = np.vstack([sess.run(samples) for _ in range(n_batches_viz)])\n",
    "#         l2nn = nn_l2_mean(x,y_ref)\n",
    "#         kl =estimate(x, y_ref,k=1)\n",
    "#         nn_dist.append(l2nn)\n",
    "#         nn_kl.append(kl)\n",
    "#         print('dist = ', l2nn)\n",
    "#         print('kl = ', kl)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_samples_ = np_samples[::1]\n",
    "cols = len(np_samples_)\n",
    "bg_color  = sns.color_palette('Greens', n_colors=256)[0]\n",
    "figure(figsize=(2*cols, 2))\n",
    "for i, samps in enumerate(np_samples_):\n",
    "    if i == 0:\n",
    "        ax = subplot(1,cols,1)\n",
    "    else:\n",
    "        subplot(1,cols,i+1, sharex=ax, sharey=ax)\n",
    "#     ax2 = sns.kdeplot(samps[:, 0], samps[:, 1], shade=True, cmap='Greens', n_levels=20, clip=[[-6,6]]*2)\n",
    "    ax2 = sns.kdeplot(samps[:, 0], samps[:, 1], shade=True, cmap='coolwarm',bw=.40, n_levels=20, clip=[[-6,6]]*2)\n",
    "\n",
    "#     ax2.set_axis_bgcolor(bg_color)\n",
    "    xticks([]); yticks([])\n",
    "    title('step %d'%(i*viz_every))\n",
    "gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(nn_dist)\n",
    "plt.semilogy(nn_kl)\n",
    "plt.legend(['kl','l2 nearest neigbhors'])\n",
    "xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('plot_con_kl',nn_kl)\n",
    "np.save('plot_con_nn',nn_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "anomaly-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
