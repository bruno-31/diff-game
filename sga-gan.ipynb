{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.contrib.kfac.python.ops.utils import fwd_gradients\n",
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "ds = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Generator and discriminator architectures\n",
    "\n",
    "(same architecture as proposed in google brain paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, output_dim=2, n_hidden=384, n_layer=6):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        h = slim.stack(z, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        x = slim.fully_connected(h, output_dim, activation_fn=None)\n",
    "    return x\n",
    "\n",
    "def discriminator(x, n_hidden=384, n_layer=6, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        h = slim.stack(x, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
    "        log_d = slim.fully_connected(h, 1, activation_fn=None)\n",
    "    return log_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mog(batch_size, n_mixture=16, std=0.2):\n",
    "    x = np.linspace(-5,4,4)\n",
    "    xs, ys = np.meshgrid(x, x)\n",
    "    xs, ys = xs.flatten(), ys.flatten()\n",
    "    cat = ds.Categorical(tf.zeros(n_mixture))\n",
    "    comps = [ds.MultivariateNormalDiag([xi, yi], [std, std]) for xi, yi in zip(xs.ravel(), ys.ravel())]\n",
    "    data = ds.Mixture(cat, comps)\n",
    "    return data.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=512,\n",
    "    disc_learning_rate=1e-5,\n",
    "    gen_learning_rate=1e-5,\n",
    "    beta1=0.5,\n",
    "    epsilon=1e-8,\n",
    "    max_iter=8000,\n",
    "    viz_every=2000,\n",
    "    z_dim=256,\n",
    "    x_dim=2,\n",
    "    optimizer='sga', # rmsprop adam sga\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Symplectic gradient adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_vec(ys,xs,vs):\n",
    "    return fwd_gradients(ys,xs,grad_xs=vs, stop_gradients=xs)\n",
    "\n",
    "def jac_tran_vec(ys,xs,vs):\n",
    "    dydxs = tf.gradients(ys,xs,grad_ys=vs, stop_gradients=xs)\n",
    "    return [tf.zeros_like(x) if dydx is None else dydx for (x,dydx) in zip(xs,dydxs)]\n",
    "\n",
    "def get_sym_adj(Ls,xs):\n",
    "    xi= [tf.gradients(l,x)[0]for(l,x)in zip(Ls,xs)]\n",
    "    H_xi = jac_vec(xi,xs,xi)\n",
    "    Ht_xi = jac_tran_vec(xi,xs,xi)\n",
    "    At_xi =[(ht-h)/2 for (h,ht) in zip(H_xi,Ht_xi)]\n",
    "    return At_xi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model and training ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data = sample_mog(params['batch_size'])\n",
    "noise = ds.Normal(tf.zeros(params['z_dim']), tf.ones(params['z_dim'])).sample(params['batch_size'])\n",
    "\n",
    "# Construct generator and discriminator nets\n",
    "with slim.arg_scope([slim.fully_connected], weights_initializer=tf.orthogonal_initializer(gain=1.4)):\n",
    "    samples = generator(noise, output_dim=params['x_dim'])\n",
    "    real_score = discriminator(data)\n",
    "    fake_score = discriminator(samples, reuse=True)\n",
    "    \n",
    "# Saddle objective    \n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=real_score, labels=tf.ones_like(real_score)) +\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.zeros_like(fake_score)))\n",
    "\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "\n",
    "\n",
    "\n",
    "if params['optimizer'] == 'rmsprop':\n",
    "    \n",
    "elif params['optimizer'] == 'adam':    \n",
    "    \n",
    "elif params['optimizer'] == 'sga':      \n",
    "# Vanilla discriminator update\n",
    "# d_train_opt = tf.train.AdamOptimizer(params['gen_learning_rate'], beta1=params['beta1'], epsilon=params['epsilon'])\n",
    "d_train_opt = tf.train.RMSPropOptimizer(params['disc_learning_rate'])\n",
    "d_train_op = d_train_opt.minimize(loss, var_list=disc_vars)\n",
    "# Vanilla generator update\n",
    "g_train_opt = tf.train.AdamOptimizer(params['gen_learning_rate'], beta1=params['beta1'], epsilon=params['epsilon'])\n",
    "d_train_opt = tf.train.RMSPropOptimizer(params['gen_learning_rate'])\n",
    "g_train_op = g_train_opt.minimize(-loss, var_list=gen_vars)\n",
    "\n",
    "else:\n",
    "    #SGA\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    gvs = optimizer.compute_gradients(cost)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'salut'\n",
    "\n",
    "if a == 'salut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = 3\n",
    "fs = []\n",
    "frames = []\n",
    "np_samples = []\n",
    "n_batches_viz = 10\n",
    "viz_every = params['viz_every']\n",
    "for i in tqdm(range(params['max_iter']+1)):\n",
    "    f, _, _ = sess.run([[loss], g_train_op, d_train_op])\n",
    "    fs.append(f)\n",
    "    if (i) % viz_every == 0:\n",
    "        print(i)\n",
    "        np_samples.append(np.vstack([sess.run(samples) for _ in range(n_batches_viz)]))\n",
    "        xx, yy = sess.run([samples, data])\n",
    "        fig = figure(figsize=(5,5))\n",
    "        scatter(xx[:, 0], xx[:, 1], edgecolor='none',s=10)\n",
    "        scatter(yy[:, 0], yy[:, 1], c='g', edgecolor='none',s=10)\n",
    "#         axis('off')\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
